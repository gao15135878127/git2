高明明	日志	20190927
1.docker容器，windows安装和linux的安装，以及一些简单的命令	完成度：安装100%，命令50%
2.文本表示：
		离散表示：完成度：90%
		分布式表示：完成度：90%
		完成度：技能50%
3.语言模型：完成度：理论60%，技能50%
		
上午开完晨会之后，丁老师先讲了windows和linux下的docker的安装教程，和一些简单的命令。
下午，徐老师讲自然语言处理和深度学习概述
文本表示：
	概述：把字符串变为向量
	离散表示：														           
			one-hot表示：是分类变量作为二进制向量的表示。这首先要求将分类值映射到整数值。然后，每个整数值被表示为二进制量，除了整数的索引之外，其它都是零值，它被标记为1.
			词袋模型：将所有词语装进一个袋子里，不考虑其词法和语序的问题，即每个词语都是独立的。
			tf-idf权重：是一种用于信息检索与数据挖掘的常用加权技术。TF意思是词频(Term Frequency)，IDF意思是逆文本频率指数(Inverse Document Frequency)。
			bi-gram和n-gram词顺序
			问题：无法衡量词向量之间的关系，词表维度随着语料库增长膨胀，n-gram词序列随语料库膨胀更快，数据稀疏问题。
	分布式表示：
			共现矩阵：Word- Document的共现矩阵主要用于发现主题( topic),用于主题模型,如LSA( Latent Semantic Analysis)。加上SVD降维，减少计算量。
			
神经网络语言模型：直接从语言模型出发，将模型最优化过程转化为求词向量表示的过程
语言模型就是计算一个单词序列（句子）的概率P的模型

Word2vec其实就是通过学习文本来用词向量的方式表征词的语义信息，即通过一个嵌入空间使得语义上相似的单词在该空间内距离很近

cbow连续词袋：2边预测中间
负例采样：如果词汇表的大小为V,那么我们就将一段长度为1的线段分成V份，每份对应词汇表中的一个词。当然每个词对应的线段长度是不一样的，高频词对应的线段长，低频词对应的线段短。然后我们从这些词中无放回的随机取出v-1个负样本。
skip-gram：中间预测2边