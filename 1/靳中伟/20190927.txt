离散数据最大的问题
	1.数据太稀疏 没办法看之前的联系
	2.随着东西的增加 内容越大

Word2Vec（NNLM升级算法：Neural Network Language Model）
	     概念：
		在大量的文本语料库中以无监督的方式学习语义知识
	     作用：
		就是让相近的语义信息中使得通过一个嵌入空间让相似单词在空间的距离很近
	     EMmbedding:
		它相对来说就是一个映射，将单词从原先所属的空间映射到高维空间去
	     词袋模型：
		就行把词语，在不考虑语法，语序的情况下打乱放入袋子中，词语都是相互独立
	     词向量模型：
		只考虑词之间的位置关系。通过大量的语料训练把该词向量映射到高纬度空间 看每个词之间的余弦值大小
		world2vec 构成的词向量模型 底层基于 CBOW 、Skip-Gram
	      skip-Gram模型
		与CBOW相反 输出的是特征词则输出的是 上下文之间的背景词，网络维度也是跟输入值相等
	     CBOW(连续词袋：bag of words)
		优点：
			1.取消了隐藏层
			2.使用双向的上下文窗口
		原理：
			CBOW的输入值就是某个特征词的上下文所对应的词向量，然而输出值就是这个特征词所对应的词向量。
			上下文词是相互对应且相等。并且不考虑其词每个词距离的大小而是考虑的是上下文之间的关系。
			CBOW会在语料库通过softmax算出概率最大的那个词 而相对来说输出对应的词向量，上下问有多少个词
			则对应的是多大的隐藏层模型，通过反向传播调整权重 输出最终的词向量
	     层次softmax：
		原理：底层基于最优化的二叉树huffmen tree:
		底层实现：根据每个词出现的频率从下往上进行建树并且在建树的过程要遵循小左大右的排序规则而且
			在分类的时候只能有两个分枝，分类完成后左0右1的编码规则如果想找单个词语对应的编码
			从下往上找 连续词则相反
	     CBOW:负例采样
		原理：因为在经行反向传播要找到相应的标签时候 总样本多 当然要选择部分语料库的东西 在选择的时候选择随机的选择
		把所有的东西 分层N等份 在词典每个单词出现的次数越多 相对来说选择的概率越大 被选择到正向类的时候者跳出
	     