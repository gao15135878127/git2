1  7个优秀的开源中文分词库推荐

（1）“结巴”中文分词，做最好的 Python 中文分词组件。

     特性
	支持三种分词模式：

	    精确模式，试图将句子最精确地切开，适合文本分析；

	    全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；

	    搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。

	支持繁体分词

	支持自定义词典

     算法

	基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)

	采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合

	对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法

（2）HanLP是一系列模型与算法组成的NLP工具包，由大快搜索主导并完全开源，目标是普及自然语言处理在生产环境中的应用。

HanLP具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。

（3）Jcseg ―― 轻量级 Java 中文分词器

（4）sego ―― Go 中文分词

sego 是一个 Go 中文分词库，词典用双数组 trie（Double-Array Trie）实现， 分词器算法为基于词频的最短路径加动态规划。

（5）FoolNLTK ―― 可能是最准的开源中文分词

（6）Ansj 中文分词 ―― 基于 n-Gram+CRF+HMM 的中文分词的 Java 实现

分词速度达到每秒钟大约200万字左右（mac air下测试），准确率能达到96%以上。目前实现了中文分词、中文姓名识别、用户自定

义词典、关键字提取、自动摘要、关键字标记等功能，可以应用到自然语言处理等方面，适用于对分词效果要求高的各种项目。

（7）word 分词 ―― Java 分布式中文分词组件

2.    Docker容器

（1）docker images  # 列出所有的镜像  

（2）docker search centos	# 查询镜像

（3）docker pull hello-world     # 默认从docker hub下载镜像

（4）docker run -it centos /bin/bash    #为centos这个镜像创建一个容器

-it就等于 -i和-t，这两个参数的作用是，为该docker创建一个伪终端，这样就是直接进入到容器里面

（5）docker rmi 镜像id/镜像名:标签名 # 删除所有的docker镜像

（6）退出容器 1.exit  容器停止退出  2. ctrl+P+Q  容器不停止退出  

     停止容器 docker kill   容器名/容器id

     强制停止容器  docker  stop  容器名/容器id

     删除已停止的容器  docker rm 容器

     重新进入容器  docker attach  容器

3.  GRU:  Rt = σ（Xt * Wxr + Ht-1 * Whr + br）  重置门

	  Zt = σ（Xt * Wxz + Ht-1 * Whz + bz）  更新门

候选隐含状态：Ht = tanh（Xt * Wxh + (Rt・Ht-1) * Whh + bh）

                                     按元素相乘 Rt=0,能够重置当前信息

最后	Ht = Zt・Ht-1 + （1-Zt）・Ht

	1                  0   drop

	Ht = Ht-1    一直保持长远的信息，不会丢失

重置门有助于捕捉时序数据中短期的依赖关系

更新门有助于捕捉时序数据中长期的依赖关系
