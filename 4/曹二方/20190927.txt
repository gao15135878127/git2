1.   LSTM  

	It = σ（Xt * Wxi + Ht-1 * Whi + bi）  输入门

	Ft = σ（Xt * Wxf + Ht-1 * Whf + bf）  遗忘门

	Ot = σ（Xt * Wxo + Ht-1 * Who + bo）  输出门


记忆细胞 Ct~ = tanh(Xt * Wxc + Ht-1 * Whc + bc)

	 Ct = Ft・Ct-1 + It・Ct~

	 Ht = Ot・tanh（Ct）

2.  文本表示

（1）定义：将字符串变为向量

（2）分类：基于粒度：长、短文本，词表示

	   基于表示方法：离散：One-hot、Multi-hot、词袋模型、TF-IDF、Bi-Gram，N-Gram

	1）词袋模型：优点：简单、方便、快捷
		
		     缺点：准确率低，不能体现不同词在不同句子中的重要性，无法关注词语之间的顺序关系

	2）TF-IDF：词在文档中的顺序没有被考虑，考虑了词的重要性

	3）Bi-Gram，N-Gram （bigram：4*10^10  trigram：8*10^15）
		
	优点：考虑词顺序    缺点：词表的膨胀

	4）离散表示的问题：无法衡量词向量之间的关系

			   词表维度随着语料库增长膨胀
		
			   n-gram词序列随语料库膨胀更快

		   	   数据稀疏问题


			分布式：用一个词附近的其他词来表示该词

				基于矩阵：降维、聚类

				基于神经网络：CBOW、Skip-gram

	1）共现矩阵：主要用于发现主题，可以挖掘语法和语义信息

		    存在问题：维度高，稀疏,空间消耗，欠稳定

		    解决：降维SVD

		    降维存在问题：计算量随语料和词典增长膨胀太快，对n*n维矩阵，计算量O（n^3）
		
				  难以为词典中新加入的词分配词向量

				  与其他深度学习模型框架差异大

	2）语言模型：一句话（词组合）出现的概率

		NNLM：直接从语言模型出发，将模型最优化过程转化为求词向量表示的过程

（3）Word2Vec：从大量文本语料中以无监督的方式学习语义知识。

	       其实就是通过学习文本来用词向量的方式表征词的语义信息，即通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。

    
模型：1）连续词袋

	     输入：无隐藏层、使用双向上下文窗口，上下文次序无关、输入层直接使用低维密度表示、投影层简化为求和（平均）

	     输出：层次Softmax：使用Huffman Tree来编码，只需计算路径上所有非叶子节点词向量的贡献，计算量降为树的深度log2（V）

		   负例采样：词典中的每一个词对应一条线段，所有词组成了[0,1]的部分

			     将[0,1]划分为M = 10^8等份，每次随机生成一个[1,M-1]的整数，看落在哪个词对应的部分上


模型：2）Skip-Gram模型
	     
	     输入：无隐藏层、投影层也可省略、每个词向量作为log-linear模型的输入	     

    embedding：将单词从原先所属的空间映射到新的多维空间中，也就是把原先词所在空间嵌入到一个新的空间中去。

问题：3） 对多义词无法很好的表示和处理，因为词向量的唯一性


3. 代码

	  


