日志： 任海峰  20190927

1.文本表示：
	不将文本视为字符串，而视为在数学上处理起来更为方便的向量。

2.为什么进行文本表示：
	（1）计算机不方便直接对文本字符串进行处理，因此需要进行数值化或者向量化。
	（2）便于机器学习。（机器学习算法需要，深度学习也需要）
	（3）良好的文本表示形式可以极大的提升算法效果。

3.（1）文本表示分类（基于粒度）：
	长文本表示 短文本表示（句子） 词表示
  （2）文本表示分类（基于表示方法）：
	离散表示：one-hot表示, Multi-hot表示, Bag of Words, TF-IDF, Bi-gram和N--gram
	分布式表示（用一个词附近的其他词来表示该词）：
		基于矩阵表示：基于降维表示 基于聚类表示
		基于神经网络：CBOW  Skip-gram

4.Bag of Words(词袋子模型):
	将字符串视为一个装满字符(词)的袋子。
  （1）优点：
	简单 方便 快捷
  （2）缺点：
	准确率低，不能体现不同词在一句话中的不同重要性。
	（如："武松打老虎"和"老虎打武松"被认为是一样的）	

5.TF-IDF：
  	文档的向量表示可以直接将各词的词向量表示加和。
	词权重（词在文档中的顺序没有被考虑）

6.Bi-gram和N-gram(常用的为bigram和trigram)：
	将两个或三个（n个）词组合在一起建立索引。
  （1）优点：
	考虑了词的顺序
  （2）缺点：
	词表的膨胀（模型参数数量呈指数上升，太惊人）

7.离散表示的问题：
	（1）无法衡量词向量之间的关系
	（2）词表维度随着语料库的增长而膨胀
	（3）n-gram词序列随语料库膨胀的更快
	（4）数据稀疏问题

8.Word-Document共现矩阵主要用于发现主题，用于主题模型，如LSA
  Word-Word共现矩阵可以挖掘语法和语义信息
	问题：
		（1）向量维度随着词典大小线性增长
		（2）存储整个词典的空间消耗非常大
		（3）一些模型如文本分类模型会面临稀疏性问题
		（4）模型会欠稳定
	解决：
		SVD降维
	SVD降维的问题：
		（1）计算量随语料和词典增长膨胀太快
		（2）难为词典中新加入的词分配词向量
		（3）与其他深度学习模型框架差异大

9.语言模型：一句话（词组合）出现的概率
  NNLM: 直接从语言模型出发，将模型最优化过程转化为求词向量表示的过程

10.Word2Vec和Embeddings:
	Word2Vec:从大量文本语料中以无监督的方式学习语义知识的一种模型。通过学习文本来用词向量的方式表征词的语义信息，通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。
	Embedding:一个映射，将单词从原先所属的空间映射到新的多维空间中，也就是把原先词所在空间嵌入到一个新的空间中去。
































	
	
