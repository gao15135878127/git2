cpu基础
构成部分： 算术/逻辑单元（执行运算） 控制单元（协调机器的活动）寄存器（临时存储）
寄存器分为：通用寄存器和专用寄存器  通用寄存器的作用是存放临时的数据  但是专用的是存放我们的指令和程序计数器   cpu 通过主存和总线进行通信 
执行两个数相加的过程： 主存读取1  主存读取2都放入相应的寄存器，将两个寄存器保存的值作为输入送到相应的加法电路，将结果保存在寄存器3中  然后将控制单元将结果放到主存中
  
指令寄存器和程序计数器
指令寄存器的作用是存储正在执行的指令程序计数器是保持下一个要执行的指令。


第一章 文本的表示

定义：什么是文本的表示------就是将文本转化成对应的向量，计算机不方便直接处理字符串

原因：
1：计算机不方便直接处理字符串
2：以便于我们的机器学习
3：良好的文本表示形式可以极大的提升算法的效果

文本表示分类有两种方式-----离散表示和分布式
离散表示：转化成onehot 编码 bow(bag of words)
分布式表示：
  基于矩阵表示：聚类和降维
  基于神经网络：cbow  和skip-gram
离散表示：
one-hot:把一句话转化成onehot 编码 缺点:高维，稀疏性太高
Bow(bagofwords):把我们的一句话分成词放进一个词袋里面，如果你想要比较两句话的相似程度就用两个袋子----但是他只关注词，但是不会关注词的先后顺序，但是比较快捷
TF-IDF:idf 就是一个权重  tf 就是一个词频 ，比如我们有一篇关于车的文档，显而易见车的频率是最高的，但是我们想要提高的是车险的那我们车险的idf 就会被提高  带入公式也是一样的
Bi-gram/n-gram  就是把一句话比如 i like playing football.... 如果分成是2gram的话 那么就是会变成  i like 1  like playing 2  playing football 3  这个样样子，从而得到[1,1,1,0.0,0]
这样以两个词就叫做2-gram 

一般我们叫  2-gram  叫做bi-gram  3-gram叫做tri-gram  5以后的基本不用了，因为模型的参数太大了
优点就是考虑了词的顺序，缺点就是词的模型参数是指数级增长的

总结离散的：要不稀疏性太高了，要不就是太容易膨胀了，最大的问题就没有办法去衡量语句之间的关系，提取词里面的语义信息


所以我们这边引用了 分布式表示（大概意思就是使用使用这个词的附近的词来表示该词）
共现矩阵：（concurrent matrix）
就是把我们的一个句子分割成为一个一个的单词的句子假设长度是n，然后我们就可以使用n*n这样的一个举证来表示我们的共现矩阵，要点是紧挨着的才可以算，前后都算



所以共现矩阵的问题是：稀疏性太大，如果词太多了也会膨胀，维度太高
所以我们怎么处理共现矩阵：  svd降维（分割成很多个小的矩阵相乘），但是依旧还是有很多计算量的问题

语言模型：
解释：就是计算出一句话出现的概率，比如我上午打篮球，踢足球，上课。。。推测出我下午----上课的概率最大（衡量一句话的合理性）

NNLM 神经网络的语言模型
输入--映射层--隐藏层--输出层

Word2vec 和embedding
Word2vec就是一个向量空间，如果是相近的，比如宾馆，旅社，旅馆  加入一层嵌入层 嵌入层的特征叫做旅馆  那么这三个就可以映射到一起

Word2vec 有两个模型，一个叫做cbow 一个叫做skip-gram
核心知识点：
Cbow:  使用前后信息去填词，使用双向的上下文窗口  输入是多少维度输出就是多少维度,
没有隐藏层
Cbow  中的层次softmax：
 使用huffman tree（也叫做最优二叉树），基本思想是，构建一个二叉树模型（然后转化为huffman 编码），然后我们想要的那个词的路径的最大概率值，也就是下图中的红色的路线，他也就是算出来的每一个分支通过sigmoid函数来表示为相应的一个概率   优点是 我们 本来输入和输出都是一样的，但是现在我们输出就变成log2（v）了，减少了数的深度
   


Cbow 的负列采样：
   一般我们的采样是随机采样，但是我们今天使用的是负列采样，就是本来我们的数据集是1亿个，我们现在就是取出500个，里面一个是正例，499个是负列，那么我们现在就相当于有一个线段，单位是1，我们现在就在样本中随机抽取，抽出一个画出一个刻度（一般都是权重大的再前面），如果抽到的是先前抽过的，就重新再抽取，如果也是正样本，那么也重新抽取，直到满足499个，
负采样的本质：：每次让一个训练样本只更新部分权重，其他权重全部固定；减少计算量；（一定程度上还可以增加随机性）


Skip-gram模型
就是由一个单词去预测周围的句子，通过bp和随机梯度下降来更新权重

Word2vec的缺点：对一个字智能表示一个含义，而不能有多重的含义

改进： grove  results  （现在用的挺多的）