岳淑君  日志  20190927：
   1、安装了cnetos7.0版本，还有docker的基本命令
   2、文本表示：把字符串变为向量是核心问题(便于处理，学习，提升算法效果)
   3、文本表示分类(基于粒度)：
	长文本表示
	短文本表示
	词表示
      文本表示分类(基于表示方法)：
	离散表示
	    One-hot表示
	    Multi-hot表示
	分布式
	    基于矩阵表示
		基于降维表示
		基于聚类表示
	    基于神经网络
		CBOW
		Skip-gram
离散表示{
   4、One-hot：缺点:稀疏、无关联
   5、Bag of Words: 优点:简单、方便、快速，前提:语料充足,任务简单，
                    缺点:准确率低，词语之间无顺序关系
   6、离散表示：TF-IDF：特点:词频、词权重，缺点：词语之间无顺序关系
   7、Bi-gram和N-gram: 优点:考虑了词的顺序 缺点:词表的膨胀
   8、离散表示的问题：
	1）无法衡量词向量之间的关系
	2）词表维度随着语料库增长膨胀
	3）n-gram此序列虽语料库增长膨胀
	4）数据稀疏问题
}
分布式表示{
   9、分布式表示(districuted representation):
	用一个此附近的其他词来表示该词
   10、共现矩阵：
	Word-Document的共现矩阵主要用于发现主题、用于主题模型，如LSA
	局域窗中的Word-Word共现矩阵可以挖掘语法和语义信息
   	存在的问题：
	1）向量维数随着词典大小线性增长
	2）存储整个词典的空间消耗非常大
	3）一些模型面临稀疏性问题
	4）模型欠稳定
   11、SVD降维：对共现矩阵向量做降维
	问题：
	1）计算量随着语料和词典增长膨胀太快
	2）难以为词典中新加入的词分配词向量
	3）与其他深度学习模型框架差异大
}
   12、语言模型：一句话（词组合）出现的概率
   13、NNLM（Neural Network Language model）
	直接从语言模型出发，将模型最优化过程转化为求词向量表示的过程
	1）使用了非对称的前向窗函数，窗长度为n-1
	2）滑动窗口遍历整个语料库求和，计算量正比于语料库大小
	3）概率满足归一化条件，这样不同位置的概率才能相加
   14、NNLM:结构：
	1）（N-1）个前向词：one-hot表示
	2）采用线性映射将one-hot表示投影到稠密D维表示
	3）输出层：softmax
	4）各层权重最优化：BP+SGD
   15、Word2Vec和Embeddings
	Word2Vec的概念：Word2Vec是从大量文本语料中以无监督的方式学习语义	知识的一种模型，它被大量的用在自然语言处理(NLP)中。Word2Vec其实		就是通过学习文本来用词向量的方式表征词的语义信息，即通过一个嵌入	空间是的语义上相似的单词在该空间内距离很近
	
	Embedding其实就是一个映射，将单词从原先所属的空间映射到新的多维	空间中，也就是把原先词所在空间嵌入到一个新的空间中去。	
   16、word2vec:CBOW(连续词袋)
	1）无隐层
	2）使用双向上下文窗口
	3）上下文次序无关
	4）输入层直接使用低维稠密表示
	5）投影层简化为求和(平均)
   17、CBOW:层次softmax
	1)使用huffman tree来编码输出层的词典
	2）只需要计算路径上所有非叶子节点词向量的贡献即可
	3）计算量降为树的深度V=>log2(V)
   18、CBOW:负例采样
	一个正样本，V-1个负样本，对付样本做采样
	损失函数：对语料库中所有词w求和
	词典中的每一个词对应一条线段，所有词组成了[0,1]间的部分
   19、word2vec:skip-gram模型
	1）无隐层
	2）投影曾也可省略
	3）每个词向量作为输入
   20、word2vec存在的问题：
	1）对每个词语单独训练，没有包含在共现矩阵中的统计信息
	2）对多义词无法很好的表示和处理，因为使用了唯一的词向量






